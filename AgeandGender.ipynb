{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMLQVeWYUc1wUDx+rY9X8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inthirmani/dooh-project-sim/blob/main/AgeandGender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrQUSlcqp7gc",
        "outputId": "2f790f33-0133-455d-bf02-79cc045797d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-04 07:33:34--  https://www.kaggle.com/api/v1/datasets/download/jangedoo/utkface-new\n",
            "Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n",
            "Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://storage.googleapis.com:443/kaggle-data-sets/44109/78156/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20251104%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251104T073334Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=49fbc3d642e252bd35fa9c13b4c743a38bb284371a621642ee98f9c62049dcd282e06b849ced8a1cceeac40938affc1d6b5b15d59838c4150c80f4a77af188e38822d8a43c9bc193715be4d90beaf8cc3a7d440b1afbaed106873081a81f739f8b29ef15070fe9c32fdc6f60b6e9ccc5530cc52c77da26dda516fea72c953a0aa8ba3de09105b9b6adc4be85d0f6a499abe9b5d8c292721b56cd0cde4f41188841b6a67037eeab59707770cdc6b2a4d3c6a5a4dea93203be20769a9bd3b1ab86fe9b98435ba1b132fdad66dd08c7ff0a59c46aa9dcb61e3b39dfc7af4ea747b1464a87fe0a2b93d3888f4e0d92a6849a3eeb7577eb3aa4875a5750af16f2ccf1 [following]\n",
            "--2025-11-04 07:33:34--  https://storage.googleapis.com/kaggle-data-sets/44109/78156/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20251104%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20251104T073334Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=49fbc3d642e252bd35fa9c13b4c743a38bb284371a621642ee98f9c62049dcd282e06b849ced8a1cceeac40938affc1d6b5b15d59838c4150c80f4a77af188e38822d8a43c9bc193715be4d90beaf8cc3a7d440b1afbaed106873081a81f739f8b29ef15070fe9c32fdc6f60b6e9ccc5530cc52c77da26dda516fea72c953a0aa8ba3de09105b9b6adc4be85d0f6a499abe9b5d8c292721b56cd0cde4f41188841b6a67037eeab59707770cdc6b2a4d3c6a5a4dea93203be20769a9bd3b1ab86fe9b98435ba1b132fdad66dd08c7ff0a59c46aa9dcb61e3b39dfc7af4ea747b1464a87fe0a2b93d3888f4e0d92a6849a3eeb7577eb3aa4875a5750af16f2ccf1\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.207, 108.177.98.207, 74.125.135.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347342542 (331M) [application/zip]\n",
            "Saving to: ‘utkface-new’\n",
            "\n",
            "utkface-new         100%[===================>] 331.25M   193MB/s    in 1.7s    \n",
            "\n",
            "2025-11-04 07:33:36 (193 MB/s) - ‘utkface-new’ saved [347342542/347342542]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# dataset\n",
        "!wget https://www.kaggle.com/api/v1/datasets/download/jangedoo/utkface-new"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q utkface-new -d /content/UTKFace/\n",
        "print(\"Dataset unzipped successfully into /content/UTKFace/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXPAGxciyYpF",
        "outputId": "209e1d4b-f83c-46c8-804e-6c9923d29b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset unzipped successfully into /content/UTKFace/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/UTKFace/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1UhTCncypIl",
        "outputId": "002ada84-b32f-48af-f0ff-32a4b01fd50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crop_part1  UTKFace  utkface_aligned_cropped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 1. Define Your Age Bins ---\n",
        "def get_age_group(age):\n",
        "    if 0 <= age <= 17:\n",
        "        return 0\n",
        "    elif 18 <= age <= 34:\n",
        "        return 1\n",
        "    elif 35 <= age <= 55:\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n",
        "\n",
        "# --- 2. Loop Through Files and Parse Labels ---\n",
        "image_paths = []\n",
        "age_labels = []\n",
        "gender_labels = []\n",
        "\n",
        "# ===================================================================\n",
        "# This path is now correct based on your 'ls' output\n",
        "directory = \"/content/UTKFace/UTKFace/\"\n",
        "# ===================================================================\n",
        "\n",
        "print(f\"Scanning directory: {directory}...\")\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    parts = filename.split('_')\n",
        "    try:\n",
        "        age = int(parts[0])\n",
        "        gender = int(parts[1]) # 0=Male, 1=Female\n",
        "\n",
        "        image_paths.append(os.path.join(directory, filename))\n",
        "        age_labels.append(get_age_group(age))\n",
        "        gender_labels.append(gender)\n",
        "\n",
        "    except Exception as e:\n",
        "        pass # Skip badly named files\n",
        "\n",
        "# --- 3. Create a clean DataFrame ---\n",
        "df = pd.DataFrame({\n",
        "    'path': image_paths,\n",
        "    'age_group': age_labels,\n",
        "    'gender': gender_labels\n",
        "})\n",
        "\n",
        "print(f\"Successfully processed {len(df)} images.\") # This should NOT be 0 now\n",
        "print(\"\\nHere's a sample of the data:\")\n",
        "print(df.head())\n",
        "\n",
        "# --- 4. Split data into Training and Validation sets ---\n",
        "if not df.empty:\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\"\\nTotal images: {len(df)}\")\n",
        "    print(f\"Training images: {len(train_df)}\")\n",
        "    print(f\"Validation images: {len(val_df)}\")\n",
        "else:\n",
        "    print(\"\\nNo images processed. Please check your 'directory' path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VKHiPFC3oyM",
        "outputId": "a8db2a78-10b5-4567-eaa6-ae2cd4c38807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning directory: /content/UTKFace/UTKFace/...\n",
            "Successfully processed 23708 images.\n",
            "\n",
            "Here's a sample of the data:\n",
            "                                                path  age_group  gender\n",
            "0  /content/UTKFace/UTKFace/61_0_1_20170117194601...          3       0\n",
            "1  /content/UTKFace/UTKFace/45_1_1_20170112215309...          2       1\n",
            "2  /content/UTKFace/UTKFace/40_0_0_20170117120447...          2       0\n",
            "3  /content/UTKFace/UTKFace/48_0_3_20170119151033...          2       0\n",
            "4  /content/UTKFace/UTKFace/35_1_1_20170117190443...          2       1\n",
            "\n",
            "Total images: 23708\n",
            "Training images: 18966\n",
            "Validation images: 4742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"Please upload your 'kaggle.json' file:\")\n",
        "files.upload()\n",
        "\n",
        "# Make the directory and move the file\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"\\nKaggle API key configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "vIAz3V3VAHx2",
        "outputId": "7170a8cf-08c9-48ae-fd10-f6d6c4379cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your 'kaggle.json' file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da2af542-3e9b-41c0-bef3-2df61d724d06\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da2af542-3e9b-41c0-bef3-2df61d724d06\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "\n",
            "Kaggle API key configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hossamrizk/cctv-gender-classifier-dataset -p /content/\n",
        "print(\"CCTV Gender Dataset downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB6UJhbMAUYN",
        "outputId": "77e9fca5-140b-49a1-823c-9cbe50dc6e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/hossamrizk/cctv-gender-classifier-dataset\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading cctv-gender-classifier-dataset.zip to /content\n",
            " 67% 152M/225M [00:00<00:00, 1.59GB/s]\n",
            "100% 225M/225M [00:00<00:00, 786MB/s] \n",
            "CCTV Gender Dataset downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/cctv-gender-classifier-dataset.zip -d /content/CCTV_Gender/\n",
        "print(\"CCTV Gender Dataset unzipped successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smmafL4WAhZr",
        "outputId": "6eceba80-d77f-4a57-8b89-516b3e4169f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CCTV Gender Dataset unzipped successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# --- Create the new CCTV DataFrame ---\n",
        "cctv_image_paths = []\n",
        "cctv_gender_labels = []\n",
        "\n",
        "# Define the paths to the new data\n",
        "# Note: The zip file has a nested folder structure\n",
        "base_cctv_dir = \"/content/CCTV_Gender/CCTV Gender Classifier Dataset/\"\n",
        "male_dir = os.path.join(base_cctv_dir, \"MALE\")\n",
        "female_dir = os.path.join(base_cctv_dir, \"FEMALE\")\n",
        "\n",
        "print(f\"Scanning {male_dir}...\")\n",
        "# Scan the MALE directory (label=0)\n",
        "for filename in os.listdir(male_dir):\n",
        "    cctv_image_paths.append(os.path.join(male_dir, filename))\n",
        "    cctv_gender_labels.append(0) # 0 = Male\n",
        "\n",
        "print(f\"Scanning {female_dir}...\")\n",
        "# Scan the FEMALE directory (label=1)\n",
        "for filename in os.listdir(female_dir):\n",
        "    cctv_image_paths.append(os.path.join(female_dir, filename))\n",
        "    cctv_gender_labels.append(1) # 1 = Female\n",
        "\n",
        "# Create a new DataFrame for the CCTV data\n",
        "cctv_df = pd.DataFrame({\n",
        "    'path': cctv_image_paths,\n",
        "    'gender': cctv_gender_labels\n",
        "})\n",
        "\n",
        "print(f\"Processed {len(cctv_df)} CCTV images.\")\n",
        "\n",
        "# --- Combine ALL our training data ---\n",
        "# We will add a 'source' column to keep track\n",
        "train_df['source'] = 'utk'\n",
        "cctv_df['source'] = 'cctv'\n",
        "\n",
        "# Now, combine them into one master training DataFrame\n",
        "# We ignore_index=True to reset the row numbers\n",
        "master_train_df = pd.concat([train_df, cctv_df], ignore_index=True)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "master_train_df = master_train_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nTotal training images (UTK + CCTV): {len(master_train_df)}\")\n",
        "print(\"Here's a sample of the combined data:\")\n",
        "print(master_train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7yLwCB1AlGk",
        "outputId": "fbbac3a3-9951-4d8c-8524-acc5f1da8e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning /content/CCTV_Gender/CCTV Gender Classifier Dataset/MALE...\n",
            "Scanning /content/CCTV_Gender/CCTV Gender Classifier Dataset/FEMALE...\n",
            "Processed 19123 CCTV images.\n",
            "\n",
            "Total training images (UTK + CCTV): 38089\n",
            "Here's a sample of the combined data:\n",
            "                                                path  age_group  gender source\n",
            "0  /content/CCTV_Gender/CCTV Gender Classifier Da...        NaN       0   cctv\n",
            "1  /content/UTKFace/UTKFace/23_1_1_20170102233446...        1.0       1    utk\n",
            "2  /content/UTKFace/UTKFace/39_0_0_20170105172311...        2.0       0    utk\n",
            "3  /content/UTKFace/UTKFace/28_0_0_20170116194200...        1.0       0    utk\n",
            "4  /content/UTKFace/UTKFace/26_0_0_20170113210605...        1.0       0    utk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# --- Step 2.2 (REVISED): Aggressive Augmentations ---\n",
        "# We use these to make UTKFace look like CCTV\n",
        "IMG_SIZE = 224\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    # 1. Simulate finding small, low-res faces.\n",
        "    # This is more realistic than just resizing.\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.4, 1.0)),\n",
        "\n",
        "    # 2. Simulate bad lighting, contrast, and saturation.\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "\n",
        "    # 3. Simulate blurry CCTV footage.\n",
        "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.0)),\n",
        "\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "\n",
        "    # 4. CRITICAL: Simulate Day/Night NoIR Camera (from your blueprint)\n",
        "    transforms.RandomGrayscale(p=0.5), # 50% chance to convert to grayscale\n",
        "\n",
        "    transforms.ToTensor(), # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]) # Normalize\n",
        "])\n",
        "\n",
        "# Validation transforms (for our val_df, which is still clean UTKFace)\n",
        "# We don't augment validation data because we want a consistent test.\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# --- Step 2.1 (REVISED): The Hybrid Dataset Class ---\n",
        "\n",
        "class HybridFaceDataset(Dataset):\n",
        "    def __init__(self, dataframe, transforms=None):\n",
        "        self.df = dataframe\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the row of data\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = row['path']\n",
        "        source = row['source']\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        # Apply our transforms\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        # Get gender (both datasets have this)\n",
        "        gender_label = torch.tensor(row['gender'], dtype=torch.long)\n",
        "\n",
        "        # Get age ONLY if it's from UTKFace\n",
        "        if source == 'utk':\n",
        "            age_label = torch.tensor(row['age_group'], dtype=torch.long)\n",
        "        else:\n",
        "            # It's from CCTV, so we use a placeholder\n",
        "            # We will tell our training loop to ignore -1\n",
        "            age_label = torch.tensor(-1, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'age_label': age_label,\n",
        "            'gender_label': gender_label\n",
        "        }\n",
        "\n",
        "# --- Create the new DataLoaders ---\n",
        "BATCH_SIZE = 32 # You can make this 64 or 128 if you have a good GPU\n",
        "\n",
        "# Use our new HybridFaceDataset for training\n",
        "# We use the 'master_train_df' which has all 38k images\n",
        "train_dataset = HybridFaceDataset(master_train_df, transforms=train_transforms)\n",
        "\n",
        "# Validation set is still the clean UTKFace 'val_df'\n",
        "# We add a 'source' column so the HybridFaceDataset class doesn't crash\n",
        "val_df['source'] = 'utk'\n",
        "val_dataset = HybridFaceDataset(val_df, transforms=val_transforms)\n",
        "\n",
        "# The DataLoaders will feed data to our model in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"\\nHybridFaceDataset class and DataLoaders are defined successfully.\")\n",
        "print(f\"Total training batches (combined): {len(train_loader)}\")\n",
        "print(f\"Total validation batches (UTK only): {len(val_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHZf8yqPBGTL",
        "outputId": "b869df72-87aa-4dc0-949f-baac8bbb23b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HybridFaceDataset class and DataLoaders are defined successfully.\n",
            "Total training batches (combined): 1191\n",
            "Total validation batches (UTK only): 149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# --- 1. Define the Multi-Task Model ---\n",
        "# This is the model with one \"body\" (MobileNetV3) and two \"heads\" (age, gender)\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, num_age_classes, num_gender_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Load the pre-trained \"body\"\n",
        "        # We use MobileNetV3-Small, as specified in the blueprint\n",
        "        self.base_model = models.mobilenet_v3_small(weights='IMAGENET1K_V1')\n",
        "\n",
        "        # 2. Get the number of input features for the classifier\n",
        "        # For MobileNetV3-Small, this is in model.classifier[0]\n",
        "        in_features = self.base_model.classifier[0].in_features\n",
        "\n",
        "        # 3. We are going to *replace* the classifier\n",
        "        # We set it to nn.Identity() which just passes the features through\n",
        "        self.base_model.classifier = nn.Identity()\n",
        "\n",
        "        # 4. Define our new \"heads\"\n",
        "        self.age_head = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_age_classes)\n",
        "        )\n",
        "\n",
        "        self.gender_head = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_gender_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Pass the image through the base model\n",
        "        features = self.base_model(x)\n",
        "\n",
        "        # 2. Pass the features to our two separate heads\n",
        "        age_logits = self.age_head(features)\n",
        "        gender_logits = self.gender_head(features)\n",
        "\n",
        "        return age_logits, gender_logits\n",
        "\n",
        "# --- 2. Initialize the Model and move it to the GPU ---\n",
        "\n",
        "# Define our constants\n",
        "NUM_AGE_CLASSES = 4   # 0-17, 18-34, 35-55, 55+\n",
        "NUM_GENDER_CLASSES = 2 # Male, Female\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create an instance of our model\n",
        "model = MultiTaskModel(NUM_AGE_CLASSES, NUM_GENDER_CLASSES)\n",
        "\n",
        "# Move the entire model to the GPU (the T4)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model created and moved to {device}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R39P4DjLB_fh",
        "outputId": "bd0cde8e-7acf-46b0-dd2b-64b8ed45ea50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.83M/9.83M [00:00<00:00, 21.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created and moved to cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm # This will give us a nice progress bar\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Define Loss Functions and Optimizer ---\n",
        "\n",
        "# For both age and gender, it's a multi-class classification problem\n",
        "# We use CrossEntropyLoss\n",
        "\n",
        "# CRITICAL: This is the \"smart\" part.\n",
        "# We tell the age loss function to IGNORE all labels that are -1.\n",
        "criterion_age = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# The gender loss is standard, as all data has a gender label.\n",
        "criterion_gender = nn.CrossEntropyLoss()\n",
        "\n",
        "# The optimizer is what updates the model's weights. Adam is a great default.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- 2. Define the Main Training Loop ---\n",
        "\n",
        "NUM_EPOCHS = 10 # We'll start with 10. You can increase this to 20 or 30 for better results.\n",
        "best_val_loss = float('inf') # We use this to track and save the best model\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    # --- TRAINING ---\n",
        "    model.train() # Set the model to training mode (enables dropout)\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    # We wrap our loader in tqdm for a progress bar\n",
        "    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
        "\n",
        "    for batch in train_progress_bar:\n",
        "        # Move data to the GPU\n",
        "        images = batch['image'].to(device)\n",
        "        age_labels = batch['age_label'].to(device)\n",
        "        gender_labels = batch['gender_label'].to(device)\n",
        "\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass (get model predictions)\n",
        "        age_logits, gender_logits = model(images)\n",
        "\n",
        "        # 3. Calculate loss\n",
        "        # The age loss will automatically ignore the -1 labels\n",
        "        loss_age = criterion_age(age_logits, age_labels)\n",
        "        loss_gender = criterion_gender(gender_logits, gender_labels)\n",
        "\n",
        "        # We combine the losses. You can weigh them if one is more important,\n",
        "        # but 1:1 is a great start.\n",
        "        total_loss = loss_age + loss_gender\n",
        "\n",
        "        # 4. Backward pass (calculate gradients)\n",
        "        total_loss.backward()\n",
        "\n",
        "        # 5. Optimizer step (update model)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update our running loss for logging\n",
        "        running_train_loss += total_loss.item()\n",
        "\n",
        "        # Update the progress bar description\n",
        "        train_progress_bar.set_postfix({'loss': running_train_loss / (train_progress_bar.n + 1)})\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "    # --- VALIDATION ---\n",
        "    model.eval() # Set the model to evaluation mode (disables dropout)\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    total_age_correct = 0\n",
        "    total_gender_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\")\n",
        "\n",
        "    with torch.no_grad(): # We don't need gradients for validation\n",
        "        for batch in val_progress_bar:\n",
        "            images = batch['image'].to(device)\n",
        "            age_labels = batch['age_label'].to(device)\n",
        "            gender_labels = batch['gender_label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            age_logits, gender_logits = model(images)\n",
        "\n",
        "            # Calculate loss (val_loader is 100% UTK, so all labels are valid)\n",
        "            loss_age = criterion_age(age_logits, age_labels)\n",
        "            loss_gender = criterion_gender(gender_logits, gender_labels)\n",
        "            total_loss = loss_age + loss_gender\n",
        "\n",
        "            running_val_loss += total_loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, age_preds = torch.max(age_logits, 1)\n",
        "            _, gender_preds = torch.max(gender_logits, 1)\n",
        "\n",
        "            total_age_correct += (age_preds == age_labels).sum().item()\n",
        "            total_gender_correct += (gender_preds == gender_labels).sum().item()\n",
        "            total_samples += age_labels.size(0)\n",
        "\n",
        "            val_progress_bar.set_postfix({'loss': running_val_loss / (val_progress_bar.n + 1)})\n",
        "\n",
        "    # Calculate average losses and accuracies\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    age_accuracy = (total_age_correct / total_samples) * 100\n",
        "    gender_accuracy = (total_gender_correct / total_samples) * 100\n",
        "\n",
        "    # --- PRINT EPOCH RESULTS ---\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} Results ---\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
        "    print(f\"  Val Age Accuracy:    {age_accuracy:.2f}%\")\n",
        "    print(f\"  Val Gender Accuracy: {gender_accuracy:.2f}%\")\n",
        "\n",
        "    # --- SAVE THE BEST MODEL ---\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"best_age_gender_model.pt\")\n",
        "        print(f\"  *** New best model saved to best_age_gender_model.pt ***\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRzHhyOxCRWQ",
        "outputId": "6582b4a9-5631-4bbd-80f0-6474c89c11bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 1191/1191 [08:12<00:00,  2.42it/s, loss=1.34]\n",
            "Epoch 1/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 10.86it/s, loss=0.944]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 1/10 Results ---\n",
            "  Train Loss: 1.3428\n",
            "  Val Loss:   0.9444\n",
            "  Val Age Accuracy:    71.68%\n",
            "  Val Gender Accuracy: 87.22%\n",
            "  *** New best model saved to best_age_gender_model.pt ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 1191/1191 [08:08<00:00,  2.44it/s, loss=1.13]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.40it/s, loss=0.903]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 2/10 Results ---\n",
            "  Train Loss: 1.1286\n",
            "  Val Loss:   0.8974\n",
            "  Val Age Accuracy:    71.00%\n",
            "  Val Gender Accuracy: 90.15%\n",
            "  *** New best model saved to best_age_gender_model.pt ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 1191/1191 [08:06<00:00,  2.45it/s, loss=1.06]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.32it/s, loss=0.829]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 3/10 Results ---\n",
            "  Train Loss: 1.0620\n",
            "  Val Loss:   0.8289\n",
            "  Val Age Accuracy:    75.07%\n",
            "  Val Gender Accuracy: 89.56%\n",
            "  *** New best model saved to best_age_gender_model.pt ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 1191/1191 [08:07<00:00,  2.44it/s, loss=1.01]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.26it/s, loss=0.861]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 4/10 Results ---\n",
            "  Train Loss: 1.0113\n",
            "  Val Loss:   0.8610\n",
            "  Val Age Accuracy:    74.15%\n",
            "  Val Gender Accuracy: 89.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 1191/1191 [08:05<00:00,  2.45it/s, loss=0.984]\n",
            "Epoch 5/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.22it/s, loss=0.812]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 5/10 Results ---\n",
            "  Train Loss: 0.9845\n",
            "  Val Loss:   0.8123\n",
            "  Val Age Accuracy:    75.79%\n",
            "  Val Gender Accuracy: 90.07%\n",
            "  *** New best model saved to best_age_gender_model.pt ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 1191/1191 [07:59<00:00,  2.48it/s, loss=0.953]\n",
            "Epoch 6/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.15it/s, loss=0.84]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 6/10 Results ---\n",
            "  Train Loss: 0.9532\n",
            "  Val Loss:   0.8399\n",
            "  Val Age Accuracy:    74.00%\n",
            "  Val Gender Accuracy: 90.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 1191/1191 [08:01<00:00,  2.47it/s, loss=0.928]\n",
            "Epoch 7/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.20it/s, loss=0.777]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 7/10 Results ---\n",
            "  Train Loss: 0.9277\n",
            "  Val Loss:   0.7718\n",
            "  Val Age Accuracy:    75.71%\n",
            "  Val Gender Accuracy: 91.61%\n",
            "  *** New best model saved to best_age_gender_model.pt ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 1191/1191 [08:01<00:00,  2.47it/s, loss=0.906]\n",
            "Epoch 8/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.44it/s, loss=0.785]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 8/10 Results ---\n",
            "  Train Loss: 0.9062\n",
            "  Val Loss:   0.7798\n",
            "  Val Age Accuracy:    75.12%\n",
            "  Val Gender Accuracy: 91.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 1191/1191 [07:59<00:00,  2.49it/s, loss=0.884]\n",
            "Epoch 9/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.02it/s, loss=0.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 9/10 Results ---\n",
            "  Train Loss: 0.8845\n",
            "  Val Loss:   0.7997\n",
            "  Val Age Accuracy:    73.79%\n",
            "  Val Gender Accuracy: 91.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 1191/1191 [08:01<00:00,  2.47it/s, loss=0.873]\n",
            "Epoch 10/10 [Val]: 100%|██████████| 149/149 [00:13<00:00, 10.96it/s, loss=0.753]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 10/10 Results ---\n",
            "  Train Loss: 0.8732\n",
            "  Val Loss:   0.7535\n",
            "  Val Age Accuracy:    76.40%\n",
            "  Val Gender Accuracy: 91.00%\n",
            "  *** New best model saved to best_age_gender_model.pt ***\n",
            "\n",
            "--- Training Complete ---\n",
            "Best validation loss: 0.7535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm # This will give us a nice progress bar\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Load Your Saved Model ---\n",
        "# First, we must re-create the model structure\n",
        "model = MultiTaskModel(NUM_AGE_CLASSES, NUM_GENDER_CLASSES)\n",
        "\n",
        "# Now, load the weights from your best-performing epoch\n",
        "model.load_state_dict(torch.load(\"best_age_gender_model.pt\"))\n",
        "\n",
        "# Move the model back to the GPU\n",
        "model.to(device)\n",
        "\n",
        "print(\"Successfully loaded 'best_age_gender_model.pt'. Resuming training...\")\n",
        "\n",
        "# --- 2. Define Loss and Optimizer (Same as before) ---\n",
        "criterion_age = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "criterion_gender = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001) # Using a slightly lower learning rate\n",
        "\n",
        "# --- 3. Run the Training Loop Again ---\n",
        "NUM_EPOCHS = 15 # Let's train for 15 more\n",
        "best_val_loss = 0.7535 # Start from your previous best loss\n",
        "\n",
        "print(f\"Continuing training for {NUM_EPOCHS} more epochs...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    # --- TRAINING ---\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+11}/{NUM_EPOCHS+10} [Train]\")\n",
        "\n",
        "    for batch in train_progress_bar:\n",
        "        images = batch['image'].to(device)\n",
        "        age_labels = batch['age_label'].to(device)\n",
        "        gender_labels = batch['gender_label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        age_logits, gender_logits = model(images)\n",
        "        loss_age = criterion_age(age_logits, age_labels)\n",
        "        loss_gender = criterion_gender(gender_logits, gender_labels)\n",
        "        total_loss = loss_age + loss_gender\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        running_train_loss += total_loss.item()\n",
        "        train_progress_bar.set_postfix({'loss': running_train_loss / (train_progress_bar.n + 1)})\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "    # --- VALIDATION ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    total_age_correct = 0\n",
        "    total_gender_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+11}/{NUM_EPOCHS+10} [Val]\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_progress_bar:\n",
        "            images = batch['image'].to(device)\n",
        "            age_labels = batch['age_label'].to(device)\n",
        "            gender_labels = batch['gender_label'].to(device)\n",
        "\n",
        "            age_logits, gender_logits = model(images)\n",
        "            loss_age = criterion_age(age_logits, age_labels)\n",
        "            loss_gender = criterion_gender(gender_logits, gender_labels)\n",
        "            total_loss = loss_age + loss_gender\n",
        "            running_val_loss += total_loss.item()\n",
        "\n",
        "            _, age_preds = torch.max(age_logits, 1)\n",
        "            _, gender_preds = torch.max(gender_logits, 1)\n",
        "\n",
        "            total_age_correct += (age_preds == age_labels).sum().item()\n",
        "            total_gender_correct += (gender_preds == gender_labels).sum().item()\n",
        "            total_samples += age_labels.size(0)\n",
        "            val_progress_bar.set_postfix({'loss': running_val_loss / (val_progress_bar.n + 1)})\n",
        "\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    age_accuracy = (total_age_correct / total_samples) * 100\n",
        "    gender_accuracy = (total_gender_correct / total_samples) * 100\n",
        "\n",
        "    # --- PRINT EPOCH RESULTS ---\n",
        "    print(f\"\\n--- Epoch {epoch+11}/{NUM_EPOCHS+10} Results ---\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
        "    print(f\"  Val Age Accuracy:    {age_accuracy:.2f}%\")\n",
        "    print(f\"  Val Gender Accuracy: {gender_accuracy:.2f}%\")\n",
        "\n",
        "    # --- SAVE THE BEST MODEL ---\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"best_age_gender_model.pt\")\n",
        "        print(f\"  *** New best model saved to best_age_gender_model.pt ***\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmCJgzeFWc_y",
        "outputId": "73d8ee83-b295-4d03-8130-ffe967a153db"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded 'best_age_gender_model.pt'. Resuming training...\n",
            "Continuing training for 15 more epochs...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/25 [Train]: 100%|██████████| 1191/1191 [08:04<00:00,  2.46it/s, loss=0.782]\n",
            "Epoch 11/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.07it/s, loss=0.7]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 11/25 Results ---\n",
            "  Train Loss: 0.7821\n",
            "  Val Loss:   0.6956\n",
            "  Val Age Accuracy:    77.94%\n",
            "  Val Gender Accuracy: 92.56%\n",
            "  *** New best model saved to best_age_gender_model.pt ***\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/25 [Train]: 100%|██████████| 1191/1191 [08:05<00:00,  2.45it/s, loss=0.743]\n",
            "Epoch 12/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.23it/s, loss=0.71]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 12/25 Results ---\n",
            "  Train Loss: 0.7427\n",
            "  Val Loss:   0.7050\n",
            "  Val Age Accuracy:    77.54%\n",
            "  Val Gender Accuracy: 92.26%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/25 [Train]: 100%|██████████| 1191/1191 [08:04<00:00,  2.46it/s, loss=0.721]\n",
            "Epoch 13/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.19it/s, loss=0.695]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 13/25 Results ---\n",
            "  Train Loss: 0.7208\n",
            "  Val Loss:   0.6952\n",
            "  Val Age Accuracy:    78.24%\n",
            "  Val Gender Accuracy: 92.43%\n",
            "  *** New best model saved to best_age_gender_model.pt ***\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/25 [Train]: 100%|██████████| 1191/1191 [07:59<00:00,  2.48it/s, loss=0.709]\n",
            "Epoch 14/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.30it/s, loss=0.704]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 14/25 Results ---\n",
            "  Train Loss: 0.7087\n",
            "  Val Loss:   0.6995\n",
            "  Val Age Accuracy:    78.03%\n",
            "  Val Gender Accuracy: 92.45%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/25 [Train]: 100%|██████████| 1191/1191 [08:00<00:00,  2.48it/s, loss=0.705]\n",
            "Epoch 15/25 [Val]: 100%|██████████| 149/149 [00:12<00:00, 11.54it/s, loss=0.711]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 15/25 Results ---\n",
            "  Train Loss: 0.7047\n",
            "  Val Loss:   0.7110\n",
            "  Val Age Accuracy:    77.86%\n",
            "  Val Gender Accuracy: 92.47%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/25 [Train]: 100%|██████████| 1191/1191 [08:02<00:00,  2.47it/s, loss=0.689]\n",
            "Epoch 16/25 [Val]: 100%|██████████| 149/149 [00:12<00:00, 11.46it/s, loss=0.694]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 16/25 Results ---\n",
            "  Train Loss: 0.6895\n",
            "  Val Loss:   0.6894\n",
            "  Val Age Accuracy:    78.76%\n",
            "  Val Gender Accuracy: 92.41%\n",
            "  *** New best model saved to best_age_gender_model.pt ***\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/25 [Train]: 100%|██████████| 1191/1191 [08:02<00:00,  2.47it/s, loss=0.679]\n",
            "Epoch 17/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.22it/s, loss=0.694]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 17/25 Results ---\n",
            "  Train Loss: 0.6786\n",
            "  Val Loss:   0.6936\n",
            "  Val Age Accuracy:    78.47%\n",
            "  Val Gender Accuracy: 92.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/25 [Train]: 100%|██████████| 1191/1191 [08:05<00:00,  2.45it/s, loss=0.677]\n",
            "Epoch 18/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.04it/s, loss=0.698]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 18/25 Results ---\n",
            "  Train Loss: 0.6770\n",
            "  Val Loss:   0.6983\n",
            "  Val Age Accuracy:    78.57%\n",
            "  Val Gender Accuracy: 92.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/25 [Train]: 100%|██████████| 1191/1191 [08:00<00:00,  2.48it/s, loss=0.672]\n",
            "Epoch 19/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.40it/s, loss=0.702]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 19/25 Results ---\n",
            "  Train Loss: 0.6720\n",
            "  Val Loss:   0.6974\n",
            "  Val Age Accuracy:    78.11%\n",
            "  Val Gender Accuracy: 92.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/25 [Train]: 100%|██████████| 1191/1191 [07:59<00:00,  2.48it/s, loss=0.665]\n",
            "Epoch 20/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.35it/s, loss=0.704]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 20/25 Results ---\n",
            "  Train Loss: 0.6646\n",
            "  Val Loss:   0.6991\n",
            "  Val Age Accuracy:    78.28%\n",
            "  Val Gender Accuracy: 92.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/25 [Train]: 100%|██████████| 1191/1191 [08:00<00:00,  2.48it/s, loss=0.652]\n",
            "Epoch 21/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 10.95it/s, loss=0.703]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 21/25 Results ---\n",
            "  Train Loss: 0.6516\n",
            "  Val Loss:   0.7030\n",
            "  Val Age Accuracy:    77.92%\n",
            "  Val Gender Accuracy: 92.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/25 [Train]: 100%|██████████| 1191/1191 [08:01<00:00,  2.48it/s, loss=0.645]\n",
            "Epoch 22/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.25it/s, loss=0.724]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 22/25 Results ---\n",
            "  Train Loss: 0.6451\n",
            "  Val Loss:   0.7191\n",
            "  Val Age Accuracy:    77.35%\n",
            "  Val Gender Accuracy: 92.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/25 [Train]: 100%|██████████| 1191/1191 [08:01<00:00,  2.47it/s, loss=0.642]\n",
            "Epoch 23/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.29it/s, loss=0.716]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 23/25 Results ---\n",
            "  Train Loss: 0.6425\n",
            "  Val Loss:   0.7108\n",
            "  Val Age Accuracy:    77.98%\n",
            "  Val Gender Accuracy: 92.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/25 [Train]: 100%|██████████| 1191/1191 [08:00<00:00,  2.48it/s, loss=0.638]\n",
            "Epoch 24/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.00it/s, loss=0.714]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 24/25 Results ---\n",
            "  Train Loss: 0.6376\n",
            "  Val Loss:   0.7139\n",
            "  Val Age Accuracy:    77.79%\n",
            "  Val Gender Accuracy: 92.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/25 [Train]: 100%|██████████| 1191/1191 [08:03<00:00,  2.47it/s, loss=0.633]\n",
            "Epoch 25/25 [Val]: 100%|██████████| 149/149 [00:13<00:00, 11.18it/s, loss=0.715]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 25/25 Results ---\n",
            "  Train Loss: 0.6328\n",
            "  Val Loss:   0.7105\n",
            "  Val Age Accuracy:    78.09%\n",
            "  Val Gender Accuracy: 92.81%\n",
            "\n",
            "--- Training Complete ---\n",
            "Best validation loss: 0.6894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your NEW 'kaggle.json' file:\")\n",
        "files.upload()\n",
        "\n",
        "# Fix the potential \"kaggle (1).json\" naming issue\n",
        "!mv kaggle*.json kaggle.json\n",
        "\n",
        "# Make the directory and move the file\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"\\nKaggle API key configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "inLJsppO6j_i",
        "outputId": "41fae8b4-d5e4-4dac-f528-a00221413275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your NEW 'kaggle.json' file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-491c3e40-1193-4882-befc-8795a5779232\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-491c3e40-1193-4882-befc-8795a5779232\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "mv: 'kaggle.json' and 'kaggle.json' are the same file\n",
            "\n",
            "Kaggle API key configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading UTKFace...\")\n",
        "!wget https://susanqq.github.io/UTKFace/data/UTKFace.tar.gz\n",
        "\n",
        "print(\"Unzipping UTKFace...\")\n",
        "# This creates the /content/UTKFace/UTKFace/ path\n",
        "!tar -xf UTKFace.tar.gz -C /content/UTKFace/\n",
        "print(\"UTKFace is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl6FNW3G7O8O",
        "outputId": "d133441b-c1ef-4fe5-acbf-0fe2dd384d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading UTKFace...\n",
            "--2025-11-04 12:52:31--  https://susanqq.github.io/UTKFace/data/UTKFace.tar.gz\n",
            "Resolving susanqq.github.io (susanqq.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to susanqq.github.io (susanqq.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-11-04 12:52:31 ERROR 404: Not Found.\n",
            "\n",
            "Unzipping UTKFace...\n",
            "tar: UTKFace.tar.gz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "UTKFace is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your 'kaggle.json' file:\")\n",
        "files.upload()\n",
        "\n",
        "# Fix the potential \"kaggle (1).json\" naming issue\n",
        "!mv kaggle*.json kaggle.json\n",
        "\n",
        "# Make the directory and move the file\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"\\nKaggle API key configured.\")\n",
        "\n",
        "# Download the CCTV dataset\n",
        "print(\"Downloading CCTV Gender dataset...\")\n",
        "!kaggle datasets download -d hossamrizk/cctv-gender-classifier-dataset -p /content/\n",
        "\n",
        "print(\"Unzipping CCTV dataset...\")\n",
        "!unzip -q /content/cctv-gender-classifier-dataset.zip -d /content/CCTV_Gender/\n",
        "print(\"CCTV dataset is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Y7fpNZpw7Shb",
        "outputId": "a6069508-d349-44d5-8d42-d788a7ba4a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your 'kaggle.json' file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a995ce3a-3072-4f59-89cf-283251e8ab50\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a995ce3a-3072-4f59-89cf-283251e8ab50\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "mv: 'kaggle.json' and 'kaggle.json' are the same file\n",
            "\n",
            "Kaggle API key configured.\n",
            "Downloading CCTV Gender dataset...\n",
            "Dataset URL: https://www.kaggle.com/datasets/hossamrizk/cctv-gender-classifier-dataset\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading cctv-gender-classifier-dataset.zip to /content\n",
            " 52% 118M/225M [00:00<00:00, 1.19GB/s]\n",
            "100% 225M/225M [00:00<00:00, 690MB/s] \n",
            "Unzipping CCTV dataset...\n",
            "CCTV dataset is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# --- 1. RE-PARSE UTKFACE (WITH CORRECT PATH) ---\n",
        "print(\"Parsing UTKFace...\")\n",
        "def get_age_group(age):\n",
        "    if 0 <= age <= 17: return 0\n",
        "    elif 18 <= age <= 34: return 1\n",
        "    elif 35 <= age <= 55: return 2\n",
        "    else: return 3\n",
        "\n",
        "image_paths = []\n",
        "age_labels = []\n",
        "gender_labels = []\n",
        "\n",
        "# ==============================================================\n",
        "# --- THIS IS THE FIX ---\n",
        "# The tar command extracted images directly into this folder\n",
        "directory = \"/content/UTKFace/\"\n",
        "# ==============================================================\n",
        "\n",
        "try:\n",
        "    for filename in os.listdir(directory):\n",
        "        parts = filename.split('_')\n",
        "        try:\n",
        "            age = int(parts[0])\n",
        "            gender = int(parts[1])\n",
        "            image_paths.append(os.path.join(directory, filename))\n",
        "            age_labels.append(get_age_group(age))\n",
        "            gender_labels.append(gender)\n",
        "        except Exception as e:\n",
        "            pass # Skip badly named files\n",
        "except FileNotFoundError:\n",
        "    print(\"UTKFace directory not found. Please re-run the download/unzip cell.\")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'path': image_paths,\n",
        "    'age_group': age_labels,\n",
        "    'gender': gender_labels\n",
        "})\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(f\"Processed {len(df)} UTKFace images.\")\n",
        "\n",
        "# --- 2. RE-PARSE CCTV DATASET ---\n",
        "print(\"Parsing CCTV Gender dataset...\")\n",
        "cctv_image_paths = []\n",
        "cctv_gender_labels = []\n",
        "base_cctv_dir = \"/content/CCTV_Gender/CCTV Gender Classifier Dataset/\"\n",
        "male_dir = os.path.join(base_cctv_dir, \"MALE\")\n",
        "female_dir = os.path.join(base_cctv_dir, \"FEMALE\")\n",
        "\n",
        "try:\n",
        "    for filename in os.listdir(male_dir):\n",
        "        cctv_image_paths.append(os.path.join(male_dir, filename))\n",
        "        cctv_gender_labels.append(0) # 0 = Male\n",
        "\n",
        "    for filename in os.listdir(female_dir):\n",
        "        cctv_image_paths.append(os.path.join(female_dir, filename))\n",
        "        cctv_gender_labels.append(1) # 1 = Female\n",
        "except FileNotFoundError:\n",
        "     print(\"CCTV directory not found. Please re-run the download/unzip cell.\")\n",
        "\n",
        "cctv_df = pd.DataFrame({\n",
        "    'path': cctv_image_paths,\n",
        "    'gender': cctv_gender_labels\n",
        "})\n",
        "print(f\"Processed {len(cctv_df)} CCTV images.\")\n",
        "\n",
        "# --- 3. COMBINE UTK + CCTV ---\n",
        "train_df['source'] = 'utk'\n",
        "cctv_df['source'] = 'cctv'\n",
        "master_train_df = pd.concat([train_df, cctv_df], ignore_index=True)\n",
        "\n",
        "# --- 4. DOWNLOAD & PARSE ADIENCE (ROBUSTLY) ---\n",
        "print(\"Attempting to download Adience dataset from Kaggle...\")\n",
        "# We run the download command and check if the zip file exists\n",
        "!kaggle datasets download -d datamunge/adience-benchmark-gender-and-age-classification -p /content/\n",
        "adience_zip_file = \"/content/adience-benchmark-gender-and-age-classification.zip\"\n",
        "adience_data = []\n",
        "adience_df = pd.DataFrame() # Create an empty dataframe\n",
        "\n",
        "if os.path.exists(adience_zip_file):\n",
        "    print(\"Adience download successful. Unzipping...\")\n",
        "    !unzip -q /content/adience-benchmark-gender-and-age-classification.zip -d /content/AdienceKaggle/\n",
        "    print(\"Adience (Kaggle) unzipped successfully.\")\n",
        "\n",
        "    print(\"Parsing Adience labels from Kaggle CSV...\")\n",
        "    def map_adience_age(age_str):\n",
        "        if age_str in ['(0, 2)', '(4, 6)', '(8, 13)', '(15, 20)']: return 0\n",
        "        elif age_str in ['(25, 32)']: return 1\n",
        "        elif age_str in ['(38, 43)', '(48, 53)']: return 2\n",
        "        elif age_str in ['(60, 100)']: return 3\n",
        "        else: return None\n",
        "    def map_adience_gender(gender_str):\n",
        "        if gender_str == 'm': return 0\n",
        "        elif gender_str == 'f': return 1\n",
        "        else: return None\n",
        "\n",
        "    label_file_path = '/content/AdienceKaggle/adience_dataset.csv'\n",
        "    try:\n",
        "        labels_df = pd.read_csv(label_file_path)\n",
        "        for index, row in labels_df.iterrows():\n",
        "            user_id, image_name = row['user_id'], row['original_image']\n",
        "            face_id, age, gender = row['face_id'], row['age'], row['gender']\n",
        "            path = f\"/content/AdienceKaggle/faces/{user_id}/landmark_aligned_face.{face_id}.{image_name}\"\n",
        "            mapped_age = map_adience_age(age)\n",
        "            mapped_gender = map_adience_gender(gender)\n",
        "            if os.path.exists(path) and mapped_age is not None and mapped_gender is not None:\n",
        "                adience_data.append({\n",
        "                    'path': path, 'age_group': mapped_age,\n",
        "                    'gender': mapped_gender, 'source': 'adience'\n",
        "                })\n",
        "        adience_df = pd.DataFrame(adience_data)\n",
        "        print(f\"Processed {len(adience_df)} valid 'in-the-wild' images from Adience.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Adience parsing failed. Continuing with 2 datasets.\")\n",
        "        pass\n",
        "else:\n",
        "    print(\"WARNING: Adience download failed (403 Forbidden). Continuing with UTKFace and CCTV only.\")\n",
        "\n",
        "# --- 5. COMBINE ALL AVAILABLE DATASETS ---\n",
        "final_train_df = pd.concat([master_train_df, adience_df], ignore_index=True)\n",
        "final_train_df = final_train_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n--- New Total Training Images ---\")\n",
        "print(f\"UTK + CCTV: {len(master_train_df)}\")\n",
        "print(f\"Adience:    {len(adience_df)}\")\n",
        "print(f\"TOTAL:      {len(final_train_df)}\")\n",
        "\n",
        "# --- 6. DEFINE AGGRESSIVE AUGMENTATIONS ---\n",
        "IMG_SIZE = 224\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.4, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomGrayscale(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "print(\"Aggressive augmentations defined.\")\n",
        "\n",
        "# --- 7. DEFINE HYBRID DATASET CLASS ---\n",
        "class HybridFaceDataset(Dataset):\n",
        "    def __init__(self, dataframe, transforms=None):\n",
        "        self.df = dataframe\n",
        "        self.transforms = transforms\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path, source = row['path'], row['source']\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "        gender_label = torch.tensor(row['gender'], dtype=torch.long)\n",
        "        if source == 'utk' or source == 'adience':\n",
        "            age_label = torch.tensor(row['age_group'], dtype=torch.long)\n",
        "        else: # source == 'cctv'\n",
        "            age_label = torch.tensor(-1, dtype=torch.long)\n",
        "        return {'image': image, 'age_label': age_label, 'gender_label': gender_label}\n",
        "\n",
        "# --- 8. CREATE FINAL DATALOADERS ---\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = HybridFaceDataset(final_train_df, transforms=train_transforms)\n",
        "val_df['source'] = 'utk'\n",
        "val_dataset = HybridFaceDataset(val_df, transforms=val_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "print(\"DataLoaders created.\")\n",
        "\n",
        "# --- 9. DEFINE THE MODEL ---\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, num_age_classes, num_gender_classes):\n",
        "        super().__init__()\n",
        "        self.base_model = models.mobilenet_v3_small(weights='IMAGENET1K_V1')\n",
        "        in_features = self.base_model.classifier[0].in_features\n",
        "        self.base_model.classifier = nn.Identity()\n",
        "        self.age_head = nn.Sequential(\n",
        "            nn.Linear(in_features, 256), nn.ReLU(),\n",
        "            nn.Dropout(0.5), nn.Linear(256, num_age_classes))\n",
        "        self.gender_head = nn.Sequential(\n",
        "            nn.Linear(in_features, 256), nn.ReLU(),\n",
        "            nn.Dropout(0.5), nn.Linear(256, num_gender_classes))\n",
        "    def forward(self, x):\n",
        "        features = self.base_model(x)\n",
        "        return self.age_head(features), self.gender_head(features)\n",
        "\n",
        "NUM_AGE_CLASSES = 4\n",
        "NUM_GENDER_CLASSES = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiTaskModel(NUM_AGE_CLASSES, NUM_GENDER_CLASSES)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"\\n--- MODEL AND DATA ARE READY! ---\")\n",
        "print(f\"Total training batches: {len(train_loader)}\")\n",
        "print(f\"Total validation batches: {len(val_loader)}\")\n",
        "print(f\"Model is on device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "nap2Lci08UHg",
        "outputId": "8559034b-dbf9-433b-f6d0-bd535fb9a768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing UTKFace...\n",
            "UTKFace directory not found. Please re-run the download/unzip cell.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1511106252.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m'gender'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgender_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m })\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processed {len(df)} UTKFace images.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    }
  ]
}